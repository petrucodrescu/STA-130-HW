{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4450d57b",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a299cbb",
   "metadata": {},
   "source": [
    "1)\n",
    "\n",
    "Simple linear regression only works with one predictor variable (the x) and analyzes the relationship between it and the outcome variable (the y). Multiple linear regression works with more than 1 predictor variable to predict and illustrate a relationship between the \"x-values\" and the outcome variable.\n",
    "\n",
    "2)\n",
    "\n",
    "When using an indicator variable for a simple linear regression, the model will predict the effect of each category on the outcome variable. Or in simpler terms, the model will create \"2 different\" lines for each indicator variable (the x-value) and will show/illustrate the correlation between it and the y-value. For multiple linear regression, it works on the same principle but now with multiple more indicator variables, which means one could analyze the relationship between the outcome variable and more predictor variables (more intuitively we can also say it makes more \"lines\").\n",
    "\n",
    "This general philosophy also works for continuous data, for a simple linear regression model, we have only one predictor variable, the x-variable that we use to model a relationship between it and the outcome variable. For multiple linear regression models, we can have multiple x-variables or multiple predictor variables allowing us to explore more complicated relationships.\n",
    "\n",
    "3)\n",
    "\n",
    "When we add an indicator variable to the model that has a continuous one or vice versa, we are able to predict a continuous/numeric trend for each group. In simpler terms, for example if we had a model that had one indicator variable and one continuous variable, we could check for one group the relationship between x and y, and then for another group the same relationship to see a difference or better model an overall relationship.\n",
    "\n",
    "4)\n",
    "\n",
    "If we were to add an interaction, the model would then allow us to not only model the groups individually but also see what effects on group could have on another group. For example if we had a line that was model with an indicator, continuous and an interaction variable, we could for example the first group model a) the relationship between it and the outcome variable and how the interaction variable also influences it, and the same for the other. We would get \"multiple\" lines to work with, each able to show a more unique relationship between the variables.\n",
    "\n",
    "5)\n",
    "\n",
    "If we had n amount of categories, we would have to create n-1 amount of binary categories as each one would: 1: Specific category or 0: other, meaning we would be able to graph/plot/model all categories without having overlap. It would behave similarly to a multiple linear regression model with only indicator variables but we would have more flexibility with how many categories we could explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1992b42",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c3d67149",
   "metadata": {},
   "source": [
    "The outcome variable would be the effectiveness of the ads and we would have 3 predictor variables. The two would be 2 continuous variables, each for the money spent on tv or online ads, and an interaction term that illustrates if they influence each other. We include an interaction variable because there could be a possible interaction with those who see the ad on TV driving online traffic for the product and leading to the online ads being more effective.\n",
    "\n",
    "The model would look like:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image-3.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "8c5d0637",
   "metadata": {},
   "source": [
    "1)\n",
    "\n",
    "Without an indicator variable:\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "With an indicator variable:\n",
    "![image-3.png](attachment:image-3.png)\n",
    "\n",
    "When using the first model, we assume that there is either no interaction or the interaction is meaningless, meaning that the outcome variable of effectiveness is solely reliant on the budgets of the 2 mediums of advertisments. The second model that uses an interaction variable assumes that they do have an influence on one another, this means that the budget would have an influence on the outcome variable (effectiveness) but also how the online and tv ads interact with each other would also affect the effectiveness of the ads.\n",
    "\n",
    "2)\n",
    "\n",
    "If we were to use binary variables such as high or low for the budget, we would replace the 2 continuous predictor variables with indicator variables, where we would say 1 means the budget is high and 0 if it is low. The model would also look like this:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "We would still keep the interaction variable as we are still interested how one group can affect the other even if we won't be comparing continuous/numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd3500e",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9bac67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.228109\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>str8fyre</td>     <th>  No. Observations:  </th>  <td>   800</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   788</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    11</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 15 Nov 2024</td> <th>  Pseudo R-squ.:     </th>  <td>0.05156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>03:40:37</td>     <th>  Log-Likelihood:    </th> <td> -182.49</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -192.41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.04757</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                      <td></td>                        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                <td>   -3.2644</td> <td>    0.714</td> <td>   -4.572</td> <td> 0.000</td> <td>   -4.664</td> <td>   -1.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>                        <td>    4.3478</td> <td>    2.179</td> <td>    1.996</td> <td> 0.046</td> <td>    0.078</td> <td>    8.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 2\") == \"None\")[T.True]</th>         <td>    1.5432</td> <td>    0.853</td> <td>    1.810</td> <td> 0.070</td> <td>   -0.128</td> <td>    3.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>                       <td>   -0.0574</td> <td>    0.468</td> <td>   -0.123</td> <td> 0.902</td> <td>   -0.975</td> <td>    0.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>                       <td>   -0.6480</td> <td>    0.466</td> <td>   -1.390</td> <td> 0.164</td> <td>   -1.561</td> <td>    0.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>                       <td>   -0.8255</td> <td>    0.545</td> <td>   -1.516</td> <td> 0.130</td> <td>   -1.893</td> <td>    0.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>                       <td>   -0.5375</td> <td>    0.449</td> <td>   -1.198</td> <td> 0.231</td> <td>   -1.417</td> <td>    0.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>                       <td>    0.3213</td> <td>    0.477</td> <td>    0.673</td> <td> 0.501</td> <td>   -0.614</td> <td>    1.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                   <td>    0.0172</td> <td>    0.006</td> <td>    3.086</td> <td> 0.002</td> <td>    0.006</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]</th>                 <td>   -0.0365</td> <td>    0.019</td> <td>   -1.884</td> <td> 0.060</td> <td>   -0.074</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                                  <td>   -0.0098</td> <td>    0.008</td> <td>   -1.247</td> <td> 0.213</td> <td>   -0.025</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:I(Q(\"Type 2\") == \"None\")[T.True]</th> <td>   -0.0197</td> <td>    0.012</td> <td>   -1.651</td> <td> 0.099</td> <td>   -0.043</td> <td>    0.004</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                           &     str8fyre     & \\textbf{  No. Observations:  } &      800    \\\\\n",
       "\\textbf{Model:}                                   &      Logit       & \\textbf{  Df Residuals:      } &      788    \\\\\n",
       "\\textbf{Method:}                                  &       MLE        & \\textbf{  Df Model:          } &       11    \\\\\n",
       "\\textbf{Date:}                                    & Fri, 15 Nov 2024 & \\textbf{  Pseudo R-squ.:     } &  0.05156    \\\\\n",
       "\\textbf{Time:}                                    &     03:40:37     & \\textbf{  Log-Likelihood:    } &   -182.49   \\\\\n",
       "\\textbf{converged:}                               &       True       & \\textbf{  LL-Null:           } &   -192.41   \\\\\n",
       "\\textbf{Covariance Type:}                         &    nonrobust     & \\textbf{  LLR p-value:       } &  0.04757    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                  & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                &      -3.2644  &        0.714     &    -4.572  &         0.000        &       -4.664    &       -1.865     \\\\\n",
       "\\textbf{Legendary[T.True]}                        &       4.3478  &        2.179     &     1.996  &         0.046        &        0.078    &        8.618     \\\\\n",
       "\\textbf{I(Q(\"Type 2\") == \"None\")[T.True]}         &       1.5432  &        0.853     &     1.810  &         0.070        &       -0.128    &        3.215     \\\\\n",
       "\\textbf{C(Generation)[T.2]}                       &      -0.0574  &        0.468     &    -0.123  &         0.902        &       -0.975    &        0.861     \\\\\n",
       "\\textbf{C(Generation)[T.3]}                       &      -0.6480  &        0.466     &    -1.390  &         0.164        &       -1.561    &        0.265     \\\\\n",
       "\\textbf{C(Generation)[T.4]}                       &      -0.8255  &        0.545     &    -1.516  &         0.130        &       -1.893    &        0.242     \\\\\n",
       "\\textbf{C(Generation)[T.5]}                       &      -0.5375  &        0.449     &    -1.198  &         0.231        &       -1.417    &        0.342     \\\\\n",
       "\\textbf{C(Generation)[T.6]}                       &       0.3213  &        0.477     &     0.673  &         0.501        &       -0.614    &        1.257     \\\\\n",
       "\\textbf{Attack}                                   &       0.0172  &        0.006     &     3.086  &         0.002        &        0.006    &        0.028     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]}                 &      -0.0365  &        0.019     &    -1.884  &         0.060        &       -0.074    &        0.001     \\\\\n",
       "\\textbf{Defense}                                  &      -0.0098  &        0.008     &    -1.247  &         0.213        &       -0.025    &        0.006     \\\\\n",
       "\\textbf{Defense:I(Q(\"Type 2\") == \"None\")[T.True]} &      -0.0197  &        0.012     &    -1.651  &         0.099        &       -0.043    &        0.004     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Logit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               str8fyre   No. Observations:                  800\n",
       "Model:                          Logit   Df Residuals:                      788\n",
       "Method:                           MLE   Df Model:                           11\n",
       "Date:                Fri, 15 Nov 2024   Pseudo R-squ.:                 0.05156\n",
       "Time:                        03:40:37   Log-Likelihood:                -182.49\n",
       "converged:                       True   LL-Null:                       -192.41\n",
       "Covariance Type:            nonrobust   LLR p-value:                   0.04757\n",
       "============================================================================================================\n",
       "                                               coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                   -3.2644      0.714     -4.572      0.000      -4.664      -1.865\n",
       "Legendary[T.True]                            4.3478      2.179      1.996      0.046       0.078       8.618\n",
       "I(Q(\"Type 2\") == \"None\")[T.True]             1.5432      0.853      1.810      0.070      -0.128       3.215\n",
       "C(Generation)[T.2]                          -0.0574      0.468     -0.123      0.902      -0.975       0.861\n",
       "C(Generation)[T.3]                          -0.6480      0.466     -1.390      0.164      -1.561       0.265\n",
       "C(Generation)[T.4]                          -0.8255      0.545     -1.516      0.130      -1.893       0.242\n",
       "C(Generation)[T.5]                          -0.5375      0.449     -1.198      0.231      -1.417       0.342\n",
       "C(Generation)[T.6]                           0.3213      0.477      0.673      0.501      -0.614       1.257\n",
       "Attack                                       0.0172      0.006      3.086      0.002       0.006       0.028\n",
       "Attack:Legendary[T.True]                    -0.0365      0.019     -1.884      0.060      -0.074       0.001\n",
       "Defense                                     -0.0098      0.008     -1.247      0.213      -0.025       0.006\n",
       "Defense:I(Q(\"Type 2\") == \"None\")[T.True]    -0.0197      0.012     -1.651      0.099      -0.043       0.004\n",
       "============================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's an example of how you can do this (i will just use the pokemon data)\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "pokeaman = pd.read_csv(url).fillna('None')\n",
    "\n",
    "pokeaman['str8fyre'] = (pokeaman['Type 1']=='Fire').astype(int)\n",
    "linear_model_specification_formula = \\\n",
    "'str8fyre ~ Attack*Legendary + Defense*I(Q(\"Type 2\")==\"None\") + C(Generation)'\n",
    "log_reg_fit = smf.logit(linear_model_specification_formula, data=pokeaman).fit()\n",
    "log_reg_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b43baf",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9163506f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VenusaurMega Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>719</td>\n",
       "      <td>Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>719</td>\n",
       "      <td>DiancieMega Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Unbound</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Dark</td>\n",
       "      <td>80</td>\n",
       "      <td>160</td>\n",
       "      <td>60</td>\n",
       "      <td>170</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>721</td>\n",
       "      <td>Volcanion</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Water</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>120</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                   Name   Type 1  Type 2  HP  Attack  Defense  \\\n",
       "0      1              Bulbasaur    Grass  Poison  45      49       49   \n",
       "1      2                Ivysaur    Grass  Poison  60      62       63   \n",
       "2      3               Venusaur    Grass  Poison  80      82       83   \n",
       "3      3  VenusaurMega Venusaur    Grass  Poison  80     100      123   \n",
       "4      4             Charmander     Fire     NaN  39      52       43   \n",
       "..   ...                    ...      ...     ...  ..     ...      ...   \n",
       "795  719                Diancie     Rock   Fairy  50     100      150   \n",
       "796  719    DiancieMega Diancie     Rock   Fairy  50     160      110   \n",
       "797  720    HoopaHoopa Confined  Psychic   Ghost  80     110       60   \n",
       "798  720     HoopaHoopa Unbound  Psychic    Dark  80     160       60   \n",
       "799  721              Volcanion     Fire   Water  80     110      120   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "0         65       65     45           1      False  \n",
       "1         80       80     60           1      False  \n",
       "2        100      100     80           1      False  \n",
       "3        122      120     80           1      False  \n",
       "4         60       50     65           1      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "795      100      150     50           6       True  \n",
       "796      160      110    110           6       True  \n",
       "797      150      130     70           6       True  \n",
       "798      170      130     80           6       True  \n",
       "799      130       90     70           6       True  \n",
       "\n",
       "[800 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "# fail https://github.com/KeithGalli/pandas/blob/master/pokemon_data.csv\n",
    "pokeaman = pd.read_csv(url) \n",
    "pokeaman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f8d50a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>3.50e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>03:40:43</td>     <th>  Log-Likelihood:    </th> <td> -3649.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   800</td>      <th>  AIC:               </th> <td>   7323.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   788</td>      <th>  BIC:               </th> <td>   7379.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                       <td>   26.8971</td> <td>    5.246</td> <td>    5.127</td> <td> 0.000</td> <td>   16.599</td> <td>   37.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>              <td>   20.0449</td> <td>    7.821</td> <td>    2.563</td> <td> 0.011</td> <td>    4.692</td> <td>   35.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>              <td>   21.3662</td> <td>    6.998</td> <td>    3.053</td> <td> 0.002</td> <td>    7.629</td> <td>   35.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>              <td>   31.9575</td> <td>    8.235</td> <td>    3.881</td> <td> 0.000</td> <td>   15.793</td> <td>   48.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>              <td>    9.4926</td> <td>    7.883</td> <td>    1.204</td> <td> 0.229</td> <td>   -5.982</td> <td>   24.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>              <td>   22.2693</td> <td>    8.709</td> <td>    2.557</td> <td> 0.011</td> <td>    5.173</td> <td>   39.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                    <td>    0.5634</td> <td>    0.071</td> <td>    7.906</td> <td> 0.000</td> <td>    0.423</td> <td>    0.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.2]</th> <td>   -0.2350</td> <td>    0.101</td> <td>   -2.316</td> <td> 0.021</td> <td>   -0.434</td> <td>   -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.3]</th> <td>   -0.3067</td> <td>    0.093</td> <td>   -3.300</td> <td> 0.001</td> <td>   -0.489</td> <td>   -0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.4]</th> <td>   -0.3790</td> <td>    0.105</td> <td>   -3.600</td> <td> 0.000</td> <td>   -0.586</td> <td>   -0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.5]</th> <td>   -0.0484</td> <td>    0.108</td> <td>   -0.447</td> <td> 0.655</td> <td>   -0.261</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.6]</th> <td>   -0.3083</td> <td>    0.112</td> <td>   -2.756</td> <td> 0.006</td> <td>   -0.528</td> <td>   -0.089</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>337.229</td> <th>  Durbin-Watson:     </th> <td>   1.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2871.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.684</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>11.649</td>  <th>  Cond. No.          </th> <td>1.40e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                  &        HP        & \\textbf{  R-squared:         } &     0.176   \\\\\n",
       "\\textbf{Model:}                          &       OLS        & \\textbf{  Adj. R-squared:    } &     0.164   \\\\\n",
       "\\textbf{Method:}                         &  Least Squares   & \\textbf{  F-statistic:       } &     15.27   \\\\\n",
       "\\textbf{Date:}                           & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  3.50e-27   \\\\\n",
       "\\textbf{Time:}                           &     03:40:43     & \\textbf{  Log-Likelihood:    } &   -3649.4   \\\\\n",
       "\\textbf{No. Observations:}               &         800      & \\textbf{  AIC:               } &     7323.   \\\\\n",
       "\\textbf{Df Residuals:}                   &         788      & \\textbf{  BIC:               } &     7379.   \\\\\n",
       "\\textbf{Df Model:}                       &          11      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                         & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                       &      26.8971  &        5.246     &     5.127  &         0.000        &       16.599    &       37.195     \\\\\n",
       "\\textbf{C(Generation)[T.2]}              &      20.0449  &        7.821     &     2.563  &         0.011        &        4.692    &       35.398     \\\\\n",
       "\\textbf{C(Generation)[T.3]}              &      21.3662  &        6.998     &     3.053  &         0.002        &        7.629    &       35.103     \\\\\n",
       "\\textbf{C(Generation)[T.4]}              &      31.9575  &        8.235     &     3.881  &         0.000        &       15.793    &       48.122     \\\\\n",
       "\\textbf{C(Generation)[T.5]}              &       9.4926  &        7.883     &     1.204  &         0.229        &       -5.982    &       24.968     \\\\\n",
       "\\textbf{C(Generation)[T.6]}              &      22.2693  &        8.709     &     2.557  &         0.011        &        5.173    &       39.366     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                    &       0.5634  &        0.071     &     7.906  &         0.000        &        0.423    &        0.703     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.2]} &      -0.2350  &        0.101     &    -2.316  &         0.021        &       -0.434    &       -0.036     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.3]} &      -0.3067  &        0.093     &    -3.300  &         0.001        &       -0.489    &       -0.124     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.4]} &      -0.3790  &        0.105     &    -3.600  &         0.000        &       -0.586    &       -0.172     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.5]} &      -0.0484  &        0.108     &    -0.447  &         0.655        &       -0.261    &        0.164     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.6]} &      -0.3083  &        0.112     &    -2.756  &         0.006        &       -0.528    &       -0.089     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 337.229 & \\textbf{  Durbin-Watson:     } &    1.505  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2871.522  \\\\\n",
       "\\textbf{Skew:}          &   1.684 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  11.649 & \\textbf{  Cond. No.          } & 1.40e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.4e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.176\n",
       "Model:                            OLS   Adj. R-squared:                  0.164\n",
       "Method:                 Least Squares   F-statistic:                     15.27\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           3.50e-27\n",
       "Time:                        03:40:43   Log-Likelihood:                -3649.4\n",
       "No. Observations:                 800   AIC:                             7323.\n",
       "Df Residuals:                     788   BIC:                             7379.\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================================\n",
       "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------\n",
       "Intercept                          26.8971      5.246      5.127      0.000      16.599      37.195\n",
       "C(Generation)[T.2]                 20.0449      7.821      2.563      0.011       4.692      35.398\n",
       "C(Generation)[T.3]                 21.3662      6.998      3.053      0.002       7.629      35.103\n",
       "C(Generation)[T.4]                 31.9575      8.235      3.881      0.000      15.793      48.122\n",
       "C(Generation)[T.5]                  9.4926      7.883      1.204      0.229      -5.982      24.968\n",
       "C(Generation)[T.6]                 22.2693      8.709      2.557      0.011       5.173      39.366\n",
       "Q(\"Sp. Def\")                        0.5634      0.071      7.906      0.000       0.423       0.703\n",
       "Q(\"Sp. Def\"):C(Generation)[T.2]    -0.2350      0.101     -2.316      0.021      -0.434      -0.036\n",
       "Q(\"Sp. Def\"):C(Generation)[T.3]    -0.3067      0.093     -3.300      0.001      -0.489      -0.124\n",
       "Q(\"Sp. Def\"):C(Generation)[T.4]    -0.3790      0.105     -3.600      0.000      -0.586      -0.172\n",
       "Q(\"Sp. Def\"):C(Generation)[T.5]    -0.0484      0.108     -0.447      0.655      -0.261       0.164\n",
       "Q(\"Sp. Def\"):C(Generation)[T.6]    -0.3083      0.112     -2.756      0.006      -0.528      -0.089\n",
       "==============================================================================\n",
       "Omnibus:                      337.229   Durbin-Watson:                   1.505\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2871.522\n",
       "Skew:                           1.684   Prob(JB):                         0.00\n",
       "Kurtosis:                      11.649   Cond. No.                     1.40e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model1_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation) + Q(\"Sp. Def\"):C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") * C(Generation)', data=pokeaman)\n",
    "\n",
    "model2_fit = model2_spec.fit()\n",
    "model2_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f4f07",
   "metadata": {},
   "source": [
    "When we create a model and calculate R^2 we are just calculating the variability that is explained by the model. The model can have a large variability but still remain a good measure of the relationship between the variables, because of this we can get into situations of high variability and strong evidence against the null hypothesis. Furthermore, the hypothesis test is not about the explained variability in the model but if there is a correlation. As long as there is a correlation and the model works as expected, getting low p-values and high variability will not be a contradiction, just more inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e31e85",
   "metadata": {},
   "source": [
    "Chatgpt Logs for Q1-Q4: https://chatgpt.com/share/673543b6-47a4-8007-ad85-4ac4b92716c2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5a159b",
   "metadata": {},
   "source": [
    "# Question 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08247fcf",
   "metadata": {},
   "source": [
    "1) This code snippet imports a new library called sklearn and then splits the data into 2 sections and trains a model on this half of the data.\n",
    "\n",
    "2) Here the model is beginning to train to fit a multiple linear regression model using our two halves of the data so we can predict the HP of a pokeaman based off its attack and defense.\n",
    "\n",
    "3) This code snippet checks how \"good\" the model is by comparing its R^2 value that it got based on the test data to that of the training data.\n",
    "\n",
    "4) The model becomes more complicated by using interaction variables to predict HP.\n",
    "\n",
    "5) And like step 3) it compares the R^2 data from the test data to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7d9396",
   "metadata": {},
   "source": [
    "# Question 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea27ce",
   "metadata": {},
   "source": [
    "When we have a model such as this, the imported library creates something called a design matrix, a table of which each row is an observation from the data, and each column is a different predictor (combination of predictors). Afterwards this is fed into the model and is used to train it, where it calculates the coefficients for each variable for each \"x-value\" to simulate a line. This now leads to the issue multicollinearity and its impact its predictive abilites for out of sample data. Since we calculate every single combination, we might lead to some combinations either being repeated or interacting so similarly with each other that it exaggerates the acutal interaction between them and hence leading to a \"skewed\" or \"inaccurate\" representation of their relationship. This then could further affect the model if we were to apply it to non-sample data, where the model could underperform when it comes to predicting certain values for certain cases as it was trained to look specifically for these highly interactive relationships that infact are just popularized because of their frequency in the design matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae75c331",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053635b3",
   "metadata": {},
   "source": [
    "Model 5 is extended from model 3 and 4 by using similar interaction and predictors but now involves the use of categorical variables to further extend the models ability to predict and model which factors influence HP. \n",
    "\n",
    "Model 6 further extends on model 5 by choosing the more impactful predictors to reduce complexity and futher reduce the lack of out of sample data generalization. (It also begins to remove collinearity\n",
    "\n",
    "Model 7 is then futher extended on model 6 by continuing to remove collinearity and improving the model's performance on out of sample data generalization. It also begins to use interaction terms to test if it would improve the model's out of sample data performance.\n",
    "\n",
    "Ultimately model 7 CS begins to use something called scaling and centering of continuous predictors to reduce multicollinearity and to improve out of sample prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "018a7cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.14771558304519894\n",
      "'Out of sample' R-squared: 0.21208501873920738\n",
      "'In sample' R-squared:     0.46709442115833855\n",
      "'Out of sample' R-squared: 0.002485342598992873\n",
      "'In sample' R-squared:     0.3920134083531893\n",
      "'Out of sample' R-squared: 0.30015614488652215\n",
      "'In sample' R-squared:     0.3326310334310908\n",
      "'Out of sample' R-squared: 0.29572460427079933\n",
      "'In sample' R-squared:     0.37818209127432456\n",
      "'Out of sample' R-squared: 0.35055389205977444\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>2.34e+09</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } & 2.34e+09  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "np.random.seed(130)\n",
    "pokeaman_train,pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train\n",
    "model_spec3 = smf.ols(formula='HP ~ Attack + Defense', \n",
    "                      data=pokeaman_train)\n",
    "model3_fit = model_spec3.fit()\n",
    "model3_fit.summary()\n",
    "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model3)[0,1]**2)\n",
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "# DO NOT try adding '* C(Generation) * C(Q(\"Type 1\")) * C(Q(\"Type 2\"))'\n",
    "# That's 6*18*19 = 6*18*19 possible interaction combinations...\n",
    "# ...a huge number that will blow up your computer\n",
    "\n",
    "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "model4_fit = model4_spec.fit()\n",
    "model4_fit.summary()\n",
    "yhat_model4 = model4_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model4)[0,1]**2)\n",
    "\n",
    "# \"Cond. No.\" WAS 343.0 WITHOUT to centering and scaling\n",
    "model3_fit.summary() \n",
    "from patsy import center, scale\n",
    "\n",
    "model3_linear_form_center_scale = \\\n",
    "  'HP ~ scale(center(Attack)) + scale(center(Defense))' \n",
    "model_spec3_center_scale = smf.ols(formula=model3_linear_form_center_scale,\n",
    "                                   data=pokeaman_train)\n",
    "model3_center_scale_fit = model_spec3_center_scale.fit()\n",
    "model3_center_scale_fit.summary()\n",
    "# \"Cond. No.\" is NOW 1.66 due to centering and scaling\n",
    "model4_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Defense))'\n",
    "model4_linear_form_CS += ' * scale(center(Speed)) * Legendary' \n",
    "model4_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# Legendary is an indicator, so we don't center and scale that\n",
    "\n",
    "model4_CS_spec = smf.ols(formula=model4_linear_form_CS, data=pokeaman_train)\n",
    "model4_CS_fit = model4_CS_spec.fit()\n",
    "model4_CS_fit.summary().tables[-1]  # Cond. No. is 2,250,000,000,000,000\n",
    "\n",
    "# The condition number is still bad even after centering and scaling\n",
    "# Just as the condition number was very bad to start with\n",
    "model4_fit.summary().tables[-1]  # Cond. No. is 12,000,000,000,000,000\n",
    "\n",
    "# Here's something a little more reasonable...\n",
    "model5_linear_form = 'HP ~ Attack + Defense + Speed + Legendary'\n",
    "model5_linear_form += ' + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "model5_linear_form += ' + C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\"))'\n",
    "\n",
    "model5_spec = smf.ols(formula=model5_linear_form, data=pokeaman_train)\n",
    "model5_fit = model5_spec.fit()\n",
    "model5_fit.summary()\n",
    "yhat_model5 = model5_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model5_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model5)[0,1]**2)\n",
    "# Here's something a little more reasonable...\n",
    "model6_linear_form = 'HP ~ Attack + Speed + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "# And here we'll add the significant indicators from the previous model\n",
    "# https://chatgpt.com/share/81ab88df-4f07-49f9-a44a-de0cfd89c67c\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model6_linear_form += ' + I(Generation==2)'\n",
    "model6_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model6_spec = smf.ols(formula=model6_linear_form, data=pokeaman_train)\n",
    "model6_fit = model6_spec.fit()\n",
    "model6_fit.summary()\n",
    "yhat_model6 = model6_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2)\n",
    "# And here's a slight change that seems to perhaps improve prediction...\n",
    "model7_linear_form = 'HP ~ Attack * Speed * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form += ' + I(Generation==2)'\n",
    "model7_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model7_spec = smf.ols(formula=model7_linear_form, data=pokeaman_train)\n",
    "model7_fit = model7_spec.fit()\n",
    "model7_fit.summary()\n",
    "yhat_model7 = model7_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2)\n",
    "# And here's a slight change that seems to perhas improve prediction...\n",
    "model7_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Speed))'\n",
    "model7_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# We DO NOT center and scale indicator variables\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form_CS += ' + I(Generation==2)'\n",
    "model7_linear_form_CS += ' + I(Generation==5)'\n",
    "\n",
    "model7_CS_spec = smf.ols(formula=model7_linear_form_CS, data=pokeaman_train)\n",
    "model7_CS_fit = model7_CS_spec.fit()\n",
    "model7_CS_fit.summary().tables[-1] \n",
    "# \"Cond. No.\" is NOW 15.4 due to centering and scaling\n",
    "# \"Cond. No.\" WAS 2,340,000,000 WITHOUT to centering and scaling\n",
    "model7_fit.summary().tables[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e109f6",
   "metadata": {},
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8892fb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.35055389205977444 (original)\n",
      "'In sample' R-squared:     0.5726118179916575 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.11151363354803218 (gen1_predict_future)\n",
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.35055389205977444 (original)\n",
      "'In sample' R-squared:     0.3904756578094535 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.23394915464343125 (gen1to5_predict_future)\n",
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.29572460427079933 (original)\n",
      "'In sample' R-squared:     0.4433880517727282 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.1932858534276128 (gen1_predict_future)\n",
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.29572460427079933 (original)\n",
      "'In sample' R-squared:     0.33517279824114776 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.26262690178799936 (gen1to5_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model7_gen1_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model7_gen1_predict_future_fit = model7_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model7_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")\n",
    "model7_gen1to5_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model7_gen1to5_predict_future_fit = model7_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model7_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")\n",
    "model6_gen1_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model6_gen1_predict_future_fit = model6_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model6_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")\n",
    "model6_gen1to5_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model6_gen1to5_predict_future_fit = model6_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model6_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5ef3f",
   "metadata": {},
   "source": [
    "This code illustrates how well the model is able to interpret data that is not its training data (ie. its out of sample generalization strength). It trains all of the models we have discussed above using a specific training data set and then proceeds to illustrate what each change (such as adding an indicator variable or combination) will do the model's ability to predict HP. Although a lower R-squared is most preferred, if the model can get a similar R^2 on the non-training data to that of the training data, one can say that the model can adjust well to new inputs and be an effective predictor of pokeaman HP. As well we can see that the models trained on more generations of pokaemon often do better as they are exposed to more variability making their training data more effective and the model's predictive power stronger."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
